# ================================================================================
# transcript-create Environment Configuration
# ================================================================================
# SECURITY NOTE: This file contains example values only. NEVER commit actual
# secrets to version control. Copy this file to .env and fill in real values.
# The .env file is gitignored and will not be committed.
#
# REQUIRED SETUP:
# 1. Copy this file: cp .env.example .env
# 2. Generate a secure SESSION_SECRET: openssl rand -hex 32
# 3. Configure DATABASE_URL if not using docker-compose default
# 4. Fill in OAuth credentials and other secrets as needed for your deployment
# ================================================================================

# --- Core Database and Worker ---
# Connection string for PostgreSQL database
# Docker compose default: postgresql+psycopg://postgres:postgres@db:5432/transcripts
# Local dev: postgresql+psycopg://postgres:postgres@localhost:5432/transcripts
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/transcripts

# Hugging Face token for speaker diarization (optional)
# Get your token from: https://huggingface.co/settings/tokens
# Leave empty to skip diarization
HF_TOKEN=

# Whisper config
WHISPER_MODEL=large-v3
WHISPER_BACKEND=faster-whisper
CHUNK_SECONDS=900
MAX_PARALLEL_JOBS=1
ROCM=true
FORCE_GPU=false
# For NVIDIA use: cuda,hip; for AMD (ROCm) prefer: hip,cuda
GPU_DEVICE_PREFERENCE=cuda,hip
GPU_COMPUTE_TYPES=float16,int8_float16,bfloat16,float32
GPU_MODEL_FALLBACKS=large-v3,medium

# --- Advanced Transcription Features ---
# Language detection and specification
# Leave empty for auto-detection, or specify language code (e.g., 'en', 'es', 'fr')
WHISPER_LANGUAGE=

# Quality presets: fast, balanced, accurate
WHISPER_QUALITY_PRESET=balanced

# Fine-grained quality controls (override preset defaults)
# Beam size for decoding (1-10, higher = more accurate but slower)
WHISPER_BEAM_SIZE=5
# Sampling temperature (0.0-1.0, 0.0 = greedy/deterministic)
WHISPER_TEMPERATURE=0.0
# Voice Activity Detection filter (faster-whisper only, reduces hallucinations)
WHISPER_VAD_FILTER=false
# Extract word-level timestamps (more precise timing data)
WHISPER_WORD_TIMESTAMPS=true

# Custom vocabulary post-processing
ENABLE_CUSTOM_VOCABULARY=true

# Translation support (experimental)
ENABLE_TRANSLATION=false
# Translation provider: 'google', 'deepl', or 'libretranslate'
TRANSLATION_PROVIDER=libretranslate
LIBRETRANSLATE_URL=https://libretranslate.com
LIBRETRANSLATE_API_KEY=
GOOGLE_TRANSLATE_API_KEY=
DEEPL_API_KEY=

# Cleanup options
CLEANUP_AFTER_PROCESS=true
CLEANUP_DELETE_RAW=true
CLEANUP_DELETE_WAV=false
CLEANUP_DELETE_CHUNKS=true
CLEANUP_DELETE_DIR_IF_EMPTY=true

# --- Search Backend ---
# Choose between 'postgres' (FTS) or 'opensearch' for search
#SEARCH_BACKEND=postgres
SEARCH_BACKEND=opensearch

# OpenSearch configuration (only used if SEARCH_BACKEND=opensearch)
OPENSEARCH_URL=http://localhost:9200
OPENSEARCH_INDEX_NATIVE=segments
OPENSEARCH_INDEX_YOUTUBE=youtube_segments

# OpenSearch authentication (leave empty for local dev with security disabled)
# For production, set strong credentials and enable security
OPENSEARCH_USER=
OPENSEARCH_INITIAL_ADMIN_PASSWORD=
OPENSEARCH_PASSWORD=

# --- Redis Cache Configuration ---
# Redis connection URL for caching layer (leave empty to disable caching)
# Docker compose default: redis://redis:6379/0
# Local dev: redis://localhost:6379/0
REDIS_URL=redis://localhost:6379/0

# Cache configuration
ENABLE_CACHING=true
CACHE_DEFAULT_TTL=300       # Default cache TTL in seconds (5 minutes)
CACHE_VIDEO_TTL=300         # Video metadata cache TTL (5 minutes)
CACHE_TRANSCRIPT_TTL=3600   # Transcript segments cache TTL (1 hour)
CACHE_SEARCH_TTL=600        # Search results cache TTL (10 minutes)

# --- Local port overrides for Docker Compose (optional) ---
# Override host ports if defaults are busy on your machine.
# Uncomment and change as needed. docker-compose picks these automatically.
# DB_HOST_PORT=5434
# API_HOST_PORT=8000
# REDIS_HOST_PORT=6379
# OPENSEARCH_HOST_PORT=9200
# DASHBOARDS_HOST_PORT=5601
# PROMETHEUS_HOST_PORT=9090
# GRAFANA_HOST_PORT=3300

# --- API / Frontend Integration ---
# CORS origin for frontend (adjust for production deployment)
FRONTEND_ORIGIN=http://localhost:5173

# --- Authentication and Sessions ---
# SECURITY CRITICAL: Generate a secure random secret for session signing
# Generate with: openssl rand -hex 32
# DO NOT use the default value in production!
SESSION_SECRET=change-me-generate-secure-random-value

# Admin dashboard access (comma-separated email addresses)
# Example: ADMIN_EMAILS=admin@example.com,manager@example.com
ADMIN_EMAILS=

# --- OAuth Configuration ---
# OAuth (Google)
# Get credentials from: https://console.cloud.google.com/apis/credentials
OAUTH_GOOGLE_CLIENT_ID=
OAUTH_GOOGLE_CLIENT_SECRET=
OAUTH_GOOGLE_REDIRECT_URI=http://localhost:8000/auth/callback/google

# OAuth (Twitch)
# Get credentials from: https://dev.twitch.tv/console/apps
OAUTH_TWITCH_CLIENT_ID=
OAUTH_TWITCH_CLIENT_SECRET=
OAUTH_TWITCH_REDIRECT_URI=http://localhost:8000/auth/callback/twitch

# --- Billing (Stripe) ---
# Get your Stripe credentials from: https://dashboard.stripe.com/apikeys
# Use test mode keys (sk_test_...) for development
# SECURITY WARNING: Never commit live keys (sk_live_...) to version control!
STRIPE_API_KEY=sk_test_...
STRIPE_PRICE_PRO_MONTHLY=price_...
STRIPE_PRICE_PRO_YEARLY=price_...

# Stripe webhook signing secret for verifying webhook events
# Get from: https://dashboard.stripe.com/webhooks
STRIPE_WEBHOOK_SECRET=whsec_...

# Redirect URLs after checkout (will substitute {origin} with FRONTEND_ORIGIN at runtime)
STRIPE_SUCCESS_URL={origin}/pricing?success=1
STRIPE_CANCEL_URL={origin}/pricing?canceled=1

# --- Logging and Observability ---
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
# Log format: 'json' for structured JSON logs, 'text' for plain text
LOG_FORMAT=json

# Enable Prometheus metrics collection
ENABLE_METRICS=true

# Optional Sentry error tracking (leave empty to disable)
SENTRY_DSN=
SENTRY_ENVIRONMENT=development
SENTRY_TRACES_SAMPLE_RATE=0.1

# --- Frontend (Vite) ---
# If unset, dev server proxies /api to http://localhost:8000
VITE_API_BASE=

# --- Quotas / Plans ---
FREE_DAILY_SEARCH_LIMIT=10
PRO_PLAN_NAME=pro
FREE_DAILY_EXPORT_LIMIT=1

# --- Security Configuration ---
# Environment: development, staging, production
ENVIRONMENT=development

# Enable rate limiting middleware (recommended for production)
ENABLE_RATE_LIMITING=true

# Session configuration (in hours)
SESSION_EXPIRE_HOURS=24
SESSION_REFRESH_THRESHOLD_HOURS=12

# API key default expiration (in days, 0 = never expires)
API_KEY_EXPIRE_DAYS=365

# OAuth security features
OAUTH_STATE_VALIDATION=true

# Additional CORS origins (comma-separated, beyond FRONTEND_ORIGIN)
# Example: CORS_ALLOW_ORIGINS=https://app.example.com,https://admin.example.com
CORS_ALLOW_ORIGINS=

# Rate limiting for failed login attempts
MAX_LOGIN_ATTEMPTS=5
LOGIN_ATTEMPT_WINDOW_MINUTES=15

# --- Backup and Disaster Recovery ---
# Root directory for all backups (database, media, WAL archives)
BACKUP_DIR=/backups

# Backup encryption with GPG (recommended for production)
BACKUP_ENCRYPT=false
BACKUP_GPG_RECIPIENT=

# Backup retention policies
BACKUP_RETENTION_DAILY=7      # Days to keep daily backups
BACKUP_RETENTION_WEEKLY=4     # Weeks to keep weekly backups (28 days)
BACKUP_RETENTION_MONTHLY=12   # Months to keep monthly backups (360 days)
MEDIA_RETENTION_DAYS=30       # Days to keep media backups

# Cloud storage for remote backups (optional - configure one or more)
# AWS S3
BACKUP_S3_BUCKET=
# Google Cloud Storage
BACKUP_GCS_BUCKET=
# Azure Blob Storage
BACKUP_AZURE_CONTAINER=

# PostgreSQL WAL archiving for Point-in-Time Recovery (PITR)
WAL_ARCHIVE_DIR=/backups/wal_archive

# Media backup directory
MEDIA_BACKUP_DIR=/backups/media

# --- PDF Export ---
PDF_FONT_PATH=
